

\documentclass[a4paper,12pt]{book}

\usepackage[spanish]{babel}
\usepackage{graphicx}




\begin{document}


	
	\begin{titlepage}
		\begin{center}
			\vspace*{-1in}
			Universidad de La Habana \\
			Facultad de Matemática y Computación \\
			Departamento de Programación \\
			\vspace*{0.15in}
			\begin{figure}[htb]
				\begin{center}
					\includegraphics[width=3cm]{./Graphics/uhlogo.pdf}
				\end{center}
			\end{figure}
			
			\vspace*{0.3in}
			\textbf{Título} \\
			\begin{large}
			Generación automática de casos de prueba con aprendizaje de la medida de calidad \\
			\end{large}
			\vspace*{0.6in}
			Autor: \textbf{Marcel Ernesto Sánchez Aguilar} \\
			Tutor: \textbf{Ludwig Leonard Méndez}
			
			
			\vspace*{0.6in}
			Trabajo de Diploma presentado en opción al título\\
			Licenciado en Ciencia de la Computación
		\end{center}
	\end{titlepage}
	
	

\chapter*{Glosario}
	\begin{itemize}
		\item Aplicación: Sistema computacional que tiene como objetivo servir de herramienta a uno o varios usuarios en la realización de diversas tareas.
		\item Aplicación de Consola: Es aquella que se ejecuta en una ventana mediante líneas de comandos.
		\item Software: Conjunto de programas y rutinas que permiten a la computadora realizar determinadas funciones.
		\item Casos de prueba: Conjunto de elementos de entrada que se utilizan para evaluar un programa.
		\item Algoritmo: Secuencia ordenada y finita de operaciones que permiten resolver un problema.
	\end{itemize}
	
\chapter{Introducción}

	La Ciencia de la Computación estudia los fundamentos teóricos de los procesos computacionales y su aplicación en la implementación de sistemas computacionales en correspondencia con el desarrollo vertiginoso de la propia ciencia y las tecnologías computacionales, los cuales dan solución a la informatización que demanda la sociedad contemporánea. El origen de esta ciencia es anterior a la invención del primer computador moderno, pues desde la antigüedad han existido procedimientos y algoritmos para realizar cómputos, solo que estos eran llevados a cabo por personas y no por un ente digital. Gracias a los trabajos de Alan Turing \cite{Turing} -considerado el padre de esta ciencia- y otros investigadores, se abrió el camino a nuevas ramas como la computabilidad y se profundizó en otras como la Inteligencia Artificial.
	
	Hoy en día la Computación ayuda al hombre en casi todas las áreas y muchas tareas que antes se hacían de forma manual, se realizan ahora mediante un equipo de cómputo. Pero, ¿qué tan buena puede ser una aplicación y cómo expresamos la calidad de las mismas? El presente trabajo se enfoca en dar solución a un subconjunto de esta problemática: evaluar el correcto funcionamiento de un algoritmo presente en una aplicación.
	
	Las aplicaciones de consola tienen una interfaz visual básica, por lo que la aceptación de las mismas recae únicamente en la lógica detrás del código. Esto es fundamental para el buen funcionamiento de cualquier programa, un correcto diseño de la lógica del código, por lo que tener un criterio de calidad respecto a una aplicación, es significativo.
	
	Una vez fijado el objetivo de evaluación, quedaría por designar las métricas a utilizar para dicho fin. ¿Cómo evaluar la aplicación? ¿En base a cuáles aspectos se mide la calidad de estos sistemas de cómputo?
	
	\section{Estado del Arte}
		%La calidad podría resultar fácil de explicar. Sin embargo, no lo es ni en su propia definición. Algunos definen la calidad como el nivel de satisfacción del cliente. Otros dicen que se trata de cumplir con los requisitos del cliente. O bien el estado del software libre de defectos. Para ello se introduce una nueva definición: deuda técnica \cite{EAsoftwareevaluation}. Esto no es más que una serie de aspectos que pueden puntuar la calidad de un software. Se han identificado ocho dimensiones de la deuda técnica del producto de software:
		
		%\begin{itemize}
			%\item Calidad del código fuente
			%\item Usabilidad, interfaz de usuario y documentación
			%\item Seguridad
			%\item Actuación
			%\item Lógica de negocios
			%\item Calidad de la arquitectura
			%\item Calidad de los datos
			%\item Uso de código fuente abierto
		%\end{itemize}
	
		%Cada dimensión se mide según las métricas críticas y los niveles que deben alcanzar. Permite recibir una evaluación integral del producto con recomendaciones concretas. Con este enfoque, los programadores entienden cómo se siente realmente el producto. Ofrece una evaluación cuantitativa completa de la calidad del producto.
		
		% deuda técnica no es solo acerca del código, este enfoque permite ejecutar un análisis profundo, el cual ha demostrado que las deudas técnicas se refieren a la calidad general del producto. Comprobar el código del software o de una de sus componentes no es suficiente para comprender la eficiencia del producto, se debe avanzar más.
		
	\section{Contexto del problema}
		
		En la Facultad de Matemática y Computación de la Universidad de La Habana, los estudiantes de primer año que cursan la asignatura de Programación de la carrera Ciencia de la Computación se enfrentan a tres evaluaciones parciales y a una prueba final. Estos exámenes tienen la peculiaridad de que se realizan a través de un equipo de cómputo. Una vez entregada las soluciones digitales (código) de los estudiantes al problema propuesto, se procede a evaluar las implementaciones de los mismos. El proceso de calificación de un examen en ninguna asignatura es trivial, por ejemplo, muchas veces lo que separa un ``buen" $3$ (a veces denominado $3+$) de un ``mal" $4$ (o $4-$) es la subjetividad del evaluador. Esto es algo extremadamente difícil de definir formalmente , pues esa subjetividad depende de los objetivos a vencer las asignaturas y de experiencias en la enseñanza. 
		
		Otro aspecto a señalar en estas pruebas particulares, es que la evaluación no se realiza directamente sobre los códigos de los estudiantes. Profesores de la asignatura se empeñan en confeccionar casos de prueba representativos que puedan dar una medida de la correctitud del algoritmo propuesto. La anterior tarea puede resultar engorrosa, diseñar esos casos de prueba puede llegar a ser incluso más difícil que la solución al problema, pues es necesario cubrir la mayor cantidad de aspectos a evaluar sin incidir de más en algún rasgo para no afectar la calificación.
		
		%El proceso de evaluación de las implementaciones de los algoritmos de los estudiantes de primer año de la carrera Ciencia de la Computación se realiza de forma semiautomática en la Facultad de Matemática y Computación de la Universidad de La Habana. La generación de los casos de prueba a utilizar es manual y puede resultar engorrosa, pues es necesario cubrir la mayor cantidad de aspectos a evaluar sin incidir de más en algún rasgo para determinar la calificación correcta de cada estudiante.
		
		Entre los principales aspectos a evaluar están la correctitud del código, el uso adecuado de los recursos del lenguaje de programación y el manejo eficiente del tiempo y la memoria (en términos computacionales).
		
%		Luego de realizada una generación abarcadora de casos de prueba, se debe escoger ese subconjunto que mejor represente la medida de calidad de la componente de software evaluada.
		
%		La idea es proveer al sistema de algunas implementaciones para las cuales se conoce de antemano las evaluaciones y luego se debe encontrar el mejor subconjunto de casos de prueba respecto a las soluciones de referencia.. Luego, si dos soluciones obtienen los mismos resultados al usar idénticos casos de prueba, es muy probable que ambas implementaciones tengan la misma calificación. Este es un problema de optimización combinatoria que será abordado con métodos de optimización meta-heurísticos.

		
		
		
	\section{Resultados esperados, importancia y herramientas a utilizar}
	
		El objetivo general de esta tesis es la generación automática de casos de prueba y la selección o ponderación de los mismos que permita utilizarlos como medida de calidad para la evaluación de algoritmos. El objeto de investigación es la optimización combinatoria abordado por meta-heurísticas \cite{OptimizacionCombinatoria} para ser aplicado en la definición de los casos de pruebas de las evaluaciones de los estudiantes de Programación en la Facultad de Matemática y Computación. Ahora bien, ¿qué algoritmo implementar? ¿Por qué una meta-heurística? ¿Será necesario aplicar un modelo de optimización? ¿Es un problema de Inteligencia Artificial o de Combinatoria? A estas y otras interrogantes se dará respuesta a lo largo de este documento.
		
		Esta solución sería extensible a cualquier algoritmo de programación. Existe abstracción sobre los algoritmos a evaluar, pero se diseñó particularmente para los tipos de exámenes que se aplican en primer año. Con solo tener en cuenta otros factores como el tiempo de ejecución y la memoria consumida, se podría ampliar el marco de algoritmos evaluables por esta propuesta.  
		
		
	
	% Objetivos, qué es lo que pienso hacer (app web, herramienta), evaluar la propuesta y compararla con otras 
	% La implementación de la propuesta sería una aplicación de escritorio. \\
	
	% Párrafo que diga la estructura del artículo
    % Este artículo se divide en... \\
	
	% Después viene trabajos relacionados (los que tratan los mismos temas que yo y se utiliza para comparar ideas) y estado del arte (análisis de lo que se ha hecho). Cerrar con una breve crítica.  \\
	% Artículos relacionados, estado del arte, ... \\
	
	% Dar una propuesta
	\section{Ejemplos}
	
	\subsection{Funcionamiento General}
	
		Consideremos el algoritmo de Euclides para hallar el máximo común divisor entre dos números.\\
		
		\textbf{Definición.} Máximo Común Divisor de dos números enteros: Es el mayor entero que los divide a ambos sin dejar resto. \\
		
		Entrada: entero $A$, entero $B$ 
		
		Método a implementar: Algoritmo de Euclides.
		
		Salida: $MCD(A, b)$ \\
		
		En este ejemplo, nuestra propuesta en la fase de aprendizaje debería generar un conjunto de entrada lo suficientemente abarcador, que permita evaluar de forma justa el algoritmo. Por ejemplo, la entrada debería contemplar el cero, los números primos, negativos, de Fibonacci, entre otros.
		
	\subsection{Funcionamiento Detallado}
	
		Consideremos el problema de la multiplicación de polinomios. Algunos aspectos a destacar serían los siguientes.
		\begin{itemize}
			\item La entrada al algoritmo serían 2 polinomios y su salida el resultado de su multiplicación.
			\item Se pasa como entrada a la propuesta un generador de polinomios y 3 respuestas al problema (implementaciones) asociadas a una nota (5, 4 o 3), además de los porcentajes de acierto esperados por cada solución. (5:100\%, 4:90\% y 3:75\% por ejemplo).
			\item La función objetivo será minimizar la sumatoria de los errores, donde error es el valor absoluto entre lo que se espera y lo obtenido en términos del porcentaje para cada nota.
		\end{itemize}
	
		Nuestra propuesta deberá generar $n$ casos aleatorios con ayuda del generador y reducirlos a un $k <= n$ representativo que se ajuste a los porcentajes esperados por cada solución. Para ello se aplicarán algoritmos metaheurísticos que se describen posteriormente.
		
	

\chapter{Marco Teórico}
	Los problemas de optimización combinatoria que involucran una extensa, pero finita, lista de posibles soluciones son muy comunes en la vida diaria. Por mencionar algunos, entre los más destacados se encuentran el diseño de redes de comunicación y la planificación de rutas de vuelo. Debido a la gran envergadura de estos problemas en cuanto a la cantidad de datos, resulta imposible enumerar las posibles soluciones y quedarnos con la mejor, pues no es factible en tiempo tal enumeración; incluso con los poderes de cómputo actuales, pues dicha lista crece de manera exponencial respecto al tamaño del problema.
	
	En los últimos 50 años se han desarrollado varios métodos de búsqueda, los cuales arrojan una solución factible cercana al óptimo del problema sin necesidad de explorar cada alternativa. Esto es lo que se conoce como Optimización Combinatoria. Los resultados han sido notorios, avances significativos en problemas de importancia para la ciencia como lo son el viajante y el enrutamiento de vehículos ya son palpables.
	
	Sin embargo, una buena parte de los problemas encontrados son computacionalmente intratables por su naturaleza o porque son lo suficientemente grandes como para impedir el uso de algoritmos exactos. En tales casos, los métodos heurísticos generalmente se emplean para encontrar soluciones buenas, pero no necesariamente óptimas. La efectividad de estos métodos depende de su capacidad para adaptarse a un ambiente particular, evitar el atrapamiento en los óptimos locales y explotar la estructura básica del problema.
	
	Sobre la base de estas nociones, se han desarrollado diversas técnicas de búsqueda basadas en heurísticas, las cuales han mejorado la capacidad de obtener buenas soluciones a diversos problemas de optimización combinatoria. Algunos de los principales métodos son: Recocido Simulado, Búsqueda Tabú, Algoritmos Genéticos y GRASP (Procedimientos de Búsqueda Adaptativa Aleatoria Codiciosos).
	
	\section{Metaheurísticas}
	
	\textbf{Definición.} Heurística: técnica o método inteligente, para realizar una tarea que no es producto de un riguroso análisis formal, sino del conocimiento experto sobre un tema a solucionar, la cual aporta soluciones con cierto grado de confianza y calidad. \\
	
	\textbf{Definición.} Método Heurístico: parte práctica del concepto de heurística. Es un enfoque para la resolución de problemas, aprendizaje o descubrimiento que emplea un método práctico no garantizado para ser óptimo o perfecto, pero suficiente para los objetivos inmediatos. \\
	
	\textbf{Definición.} Metaheurística: son una clase de métodos aproximados que están diseñados para resolver problemas difíciles de optimización combinatoria, en los que los heurísticos clásicos no son efectivos. Las metaheurísticas proporcionan un marco general para crear nuevos algoritmos híbridos combinando diferentes conceptos derivados de la inteligencia artificial, la evolución biológica y los procedimientos estadísticos. \\
	
	Las metaheurísticas generalmente se aplican cuando se desconoce de un algoritmo que resuelva de manera satisfactoria dichos problemas. Muchas veces lo anterior se debe a que no es factible explorar en su totalidad el campo de soluciones en busca de un óptimo global, por lo que se emplean heurísticas o métodos aproximados. Los problemas de optimización combinatoria son usuales escenarios en la aplicación de una metaheurística.
	
	La optimización combinatoria se basa en encontrar una configuración de bits (selección), respecto a la entrada del problema, que maximice o minimice una función objetivo específica. A los pasos intermedios anteriores a la solución se les denomina estados, y al conjunto de todos los estados candidatos se le llama espacio de búsqueda. La función objetivo, los estados y el espacio de búsqueda son definidos en función del problema.
	
	Existen metaheurísticas que mantienen un único estado actual durante cada instante de ejecución, el cual es actualizado en cada iteración. Este paso se conoce como función de transición. Otras metaheurísticas más sofisticadas mantienen, en vez de un único estado actual, un conjunto de estados candidatos. Así, la función de transición añade o elimina estados de este conjunto. Otros procedimientos pueden guardar información del óptimo actual, escogiendo el estado óptimo entre todos los óptimos locales obtenidos en varias etapas del algoritmo.
	
	Como se hizo mención anteriormente, el espacio de búsqueda puede resultar extremadamente grande o incluso infinito, por lo cual es necesario definir algunos criterios de parada para la ejecución del algoritmo. Entre los más usuales podemos encontrar el efectuar un número de iteraciones especificadas por el usuario, el alcance de un determinado tiempo de ejecución o el cumplimiento de una condición específica del problema. \\
	
	Existen muchos métodos heurísticos con comportamientos y objetivos diferentes, por lo que resulta complicado clasificarlos. Esto se debe en parte a que muchos de  ellos  han  sido  diseñados para un problema específico sin posibilidad de generalización o aplicación a otros problemas similares. Sin embargo, algunas situaciones pueden resultar tener un parecido en cuanto al tipo de idea a seguir para su solución, de ahí surge una especie de clasificación entre estos algoritmos.
	
	\begin{itemize}
		\item Métodos de Descomposición: son aquellos aplicables a problemas que se descomponen en varios subproblemas más sencillos de resolver que el original.
		
		\item Métodos Inductivos: la idea es generalizar versiones pequeñas o más sencillas al caso completo. Propiedades o técnicas identificadas en estos casos más fáciles de analizar pueden ser aplicadas al problema completo.
		
		\item Métodos de Reducción: consiste en identificar propiedades que cumplen principalmente las buenas soluciones e introducirlas como restricciones del problema.  El objetivo es restringir el espacio de soluciones al simplificar el problema. Se corre el riesgo de dejar fuera soluciones óptimas del problema original.
		
		\item Métodos Constructivos: se caracterizan por construir en cada paso una solución del problema. Usualmente son  métodos deterministas y suelen estar basados en la mejor elección en cada iteración.
		
		\item Métodos de Búsqueda Local: los  procedimientos  de  búsqueda o mejora local comienzan con una solución del problema y la mejoran progresivamente.  El procedimiento realiza en cada paso un movimiento de una solución a otra con mejor valor. El método finaliza cuando, para una solución, no existe ninguna solución accesible que la mejore.	\end{itemize}
	
		Los métodos constructivos y los de búsqueda local resaltan entre los procedimientos metaheurísticos más empleados y con mejores resultados en la práctica, por lo que para la confección de nuestra propuesta fueron seleccionados algunos de los que resultan aplicables a nuestro problema. 
	
	
		\subsection{Greedy Randomized Adaptive Search Procedures} 
		GRASP \cite{GRASP} es una técnica de muestreo aleatorio iterativo. Tiene la invariante de que en cada iteración proporciona una solución factible al problema en cuestión. Hay dos fases dentro de cada iteración del algoritmo: la primera construye inteligentemente una solución a través de una función codiciosa aleatoria; la segunda aplica un procedimiento de búsqueda local a la solución construida con la esperanza de encontrar una mejora. Esto se repite mientras no se alcance una condición de parada, que pudiera ser el cumplimiento de un número de iteraciones o el alcance de un valor en la función objetivo.
		
		En la Figura \ref{GRASPgeneral} se muestra el procedimiento general del GRASP
		
		\begin{figure}[h]
			\centering
			\includegraphics[width=10cm]{./Graphics/GRASPgeneral.png}
			\caption{Algoritmo General GRASP}
			\label{GRASPgeneral}
		\end{figure}
		
		Ahora se construye iterativamente una solución factible. En cada iteración de construcción, la elección del siguiente elemento a agregar se determina respecto a una función codiciosa. Para reflejar los cambios provocados por la selección del elemento anterior, los beneficios asociados con cada elemento se actualizan en cada iteración. El componente probabilístico de un GRASP se caracteriza por elegir aleatoriamente uno de los candidatos de la lista, pero no necesariamente el mejor candidato. La lista de los candidatos se denomina lista de candidatos restringidos (RCL). Esta técnica de elección permite obtener diferentes soluciones en cada iteración GRASP.
		
		La Figura \ref{GRASPconstructionphase} muestra el pseudocódigo para la fase de construcción de GRASP.
	
		\begin{figure}[h]
			\centering
			\includegraphics[width=10cm]{./Graphics/GRASPconstructionphase.png}
			\caption{Construcción de la solución GRASP}
			\label{GRASPconstructionphase}
		\end{figure}
	
		El algoritmo de búsqueda local es iterativo y reemplaza sucesivamente la solución actual por una mejor en la vecindad. Termina cuando no se encuentra una solución mejor en el vecindario. La clave del éxito para un algoritmo de búsqueda local consiste en la elección adecuada de una estructura de la vecindad, técnicas eficientes de búsqueda y la solución inicial.
		
		A continuación, la Figura \ref{GRASPlocal} muestra dicho procedimiento.
		
		\begin{figure}[h]
			\centering
			\includegraphics[width=10cm]{./Graphics/GRASPlocal.png}
			\caption{Búsqueda local GRASP}
			\label{GRASPlocal}
		\end{figure}
	
	\subsection{Recocido Simulado}
		Kirkpatrick, Gelatt y Vecchi (1983) e independientemente Cerny (1985) propusieron un nuevo enfoque para la solución aproximada de problemas de optimización combinatoria. Este enfoque, Recocido Simulado \cite{RecocidoSimulado} (Simulated Annealing) está motivado por una analogía con el comportamiento de los sistemas físicos en presencia de un baño de calor. El enfoque no físico puede verse como una versión mejorada de la técnica de optimización local o mejora iterativa, en la que una solución inicial se mejora repetidamente haciendo pequeñas alteraciones locales hasta que dicha alteración no produzca una mejor solución. El Recocido Simulado aleatoriza este procedimiento de una manera que permite movimientos ascendentes ocasionales (cambios que empeoran la solución), en un intento de reducir la probabilidad de quedar atrapado en una solución localmente óptima. El Recocido Simulado se puede adaptar fácilmente a nuevos problemas (incluso en ausencia de una comprensión profunda de los problemas mismos) y, debido a su aparente capacidad para evitar los óptimos locales deficientes, ofrece la esperanza de obtener resultados significativamente mejores.
		
		Para comprender el Recocido Simulado, primero se debe entender la optimización local. Se puede especificar un problema de optimización combinatoria identificando un conjunto de soluciones junto con una función de costo que asigna un valor numérico a cada solución. Una solución óptima es una solución con el mínimo costo posible (puede haber más de una solución de este tipo). Dada una solución arbitraria a tal problema, la optimización local intenta mejorar esa solución mediante una serie de cambios locales incrementales. Para definir un algoritmo de optimización local, primero se especifica un método para perturbar las soluciones para obtener otras. El conjunto de soluciones que se pueden obtener en uno de esos pasos a partir de una solución dada A se llama vecindad de A. El algoritmo luego realiza el ciclo simple que se muestra en la Figura \ref{OptLocalRS}, con los métodos específicos para elegir $S$ y $S'$ como detalles de implementación.
		
		\begin{figure}[h]
		\centering
		\includegraphics[width=10cm]{./Graphics/RecocidoSimuladoOptLocal.png}
		\caption{Optimización local}
		\label{OptLocalRS}
		\end{figure}
		
		Aunque S no necesita ser una solución óptima cuando finalmente se cierra el ciclo, será localmente óptima ya que ninguno de sus vecinos tiene un costo menor. La esperanza es que localmente óptimo sea lo suficientemente bueno. \\
		
		La dificultad de la optimización local es que no tiene forma de retirarse de los óptimos locales poco atractivos. Nunca se pasa a una nueva solución a menos que la dirección sea cuesta abajo, es decir, a un mejor valor de la función de costo. El Recocido Simulado es un enfoque que intenta evitar dicho atrapamiento. Esto se realiza bajo la influencia de un generador de números aleatorios y un parámetro de control llamado temperatura. Como se implementa típicamente, el enfoque de Recocido Simulado involucra un par de ciclos anidados y dos parámetros adicionales, una relación de enfriamiento $r$, $0 < r <1$, y una longitud de temperatura entera $L$ (ver Figura \ref{RecocidoSimulado}). En el Paso 3 del algoritmo, el término congelado se refiere a un estado en el que no parece probable una mejora adicional en el costo ($S$).
		
		\begin{figure}[h]
		\centering
		\includegraphics[width=10cm]{./Graphics/RecocidoSimulado.png}
		\caption{RecocidoSimulado}
		\label{RecocidoSimulado}
		\end{figure}
		
		$e ^{-\Delta / T}$ será un número en el intervalo $(0, 1)$ donde $\Delta$ y $T$ son positivos. La probabilidad de que un movimiento cuesta arriba de tamaño $\Delta$ sea aceptado disminuye proporcionalmente a la temperatura y, para una temperatura fija $T$, los movimientos ascendentes pequeños tienen mayores probabilidades de aceptación que los grandes. Este método particular de operación está motivado por una analogía física.
		
	\subsection{Algoritmos Genéticos}
		Un Algoritmo Genético \cite{AlgGen} consiste en un conjunto de soluciones codificadas, que hacen una analogía con los cromosomas.  Cada uno  de  estos tendrá  asociado  un  ajuste o valor de bondad, que expresa una medida de su valor como solución al problema. En función de este valor se le darán más o menos oportunidades de ``reproducción". John Holland, investigador de la Universidad de Michigan, es uno de los principales precursores del desarrollo de los Algoritmos Genéticos. Sus trabajos a finales de la década de los 60 mostraron una técnica que imitaba en su funcionamiento a la selección natural.
		
		La reproducción en estos algoritmos puede darse de dos formas:
		\begin{itemize}
			\item Cruce: Se genera una descendencia a partir del mismo número de individuos (generalmente 2) de la generación anterior. 
			\item Copia: Un determinado número de individuos pasa sin sufrir ninguna variación directamente a la siguiente generación.
		\end{itemize}
	
		La Figura \ref{AlgoritmoeGenetico} muestra el funcionamiento de un algoritmo genético
	
		\begin{figure}[h]
			\centering
			\includegraphics[width=10cm]{./Graphics/AlgoritmoGenetico.png}
			\caption{Algoritmo Genético}
			\label{AlgoritmoeGenetico}
		\end{figure}
	
		Algunos de los criterios de parada pudieran ser:
		\begin{itemize}
			\item Se ha alcanzado una población con individuos lo suficientemente buenos para darle solución al problema.
			\item Ha convergido la población, lo cual quiere decir que la  media  de  bondad  de  la  misma  se aproxima a la bondad del mejor individuo.
			\item Se ha alcanzado el número de generaciones (iteraciones) especificado.
		\end{itemize}
	
		A este algoritmo se le han definido numerosas variantes. Una de las más extendidas es aplicar los operadores genéticos de cruce y copia directamente sobre la población genética. Pero en el caso de que se aplique cruce, no se puede insertar directamente la descendencia en la población, debido a que el número de individuos de la población se ha de mantener constante. Es decir, para permitir a los descendientes generados incorporarse a la población, se  han  de  eliminar  otros individuos. Trabajando con una sola población no se puede definir que la misma está llena, pues el número de sus miembros es constante. En este caso se pasará a la siguiente población cuando se hayan alcanzado un número determinado de cruzamientos especificados por el usuario, que deberán estar acordes al tamaño de la población.
		
		%En el esquema general, solo los descendientes originados a partir de un cruce son mutados. Otra opción habitual es la selección aleatoria del individuo a mutar entre todos los que forman parte de la población.
		
		
\chapter{Diseño de la Propuesta}

	Para el diseño lógico de esta herramienta se utilizará la aplicación Visual Studio y el lenguaje de programación C\#. Esta elección se debe a las facilidades que ambos brindan para la confección de una solución a este problema. Pero pudiera haberse utilizado cualquier lenguaje de propósito general. 
	Para poder aplicar alguna metahehurística al problema que se intenta dar solución, es necesario definir nuestro espacio de búsqueda, así como las entradas, salidas y adaptaciones hechas a las implementaciones.
	
	Se provee al sistema de un conjunto inicial de casos de prueba brindados por el generador, los cuales inicialmente constituyen una solución, pues nuestro problema de optimización no tiene restricciones, solo una función objetivo $f(x,y)$ a minimizar: \\
	
	$ f(x,y) = |x - x_0| + |y - y_0|$ 
	
	\begin{itemize}
		\item $x$: Porcentaje de acierto obtenido por la solución con nota 3
		\item $x_0$: Porcentaje de acierto esperado de la solución con nota 3
		\item $y$: Porcentaje de acierto obtenido por la solución con nota 4
		\item $y_0$: Porcentaje de acierto esperado de la solución con nota 4
	\end{itemize}

	La solución que brinde el algoritmo debe consistir en un subconjunto del conjunto inicial de casos de prueba, que debería tener una evaluación menor o igual que la inicial. Se ha de aclarar que la salida puede estar sesgada por la entrada, es decir, que el conjunto que se reciba como entrada no sea una muestra representativa de la población de casos de prueba.
	
	A continuación se explican las adaptaciones hechas a los dos algoritmos implementados: GRASP y Genético, los cuales reciben la misma entrada y aplican un proceder diferente.
	
	\section{Adaptación de GRASP}
		Como mencionábamos en (hacer referencia a la subsección), GRASP en cada iteración hace una búsqueda local, la cual brinda otra solución factible al algoritmo. En nuestro problema en particular, se tienen inicialmente los $n$ casos de prueba, de los cuales se selecciona un subconjunto de tamaño $k, k \leq n$ que será la lista de candidatos de eliminar, los cuales representan una mejora (menor evaluación) de la función objetivo si no se les considerara en la solución final del problema. Una vez conformada dicha selección se toma uno de la misma al azar y se retira, posteriormente se hace otra iteración del algoritmo pero ahora con un conjunto de tamaño $n-1$.
		
		Como casos de parada tenemos los siguientes:
		\begin{itemize}
			\item Se ha alcanzado el valor esperado de la función objetivo.
			\item Se han cumplido un número de iteraciones especificadas.
			\item La lista de candidatos a extraer es vacía.
		\end{itemize}
	
		Para un mayor rendimiento de este algoritmo, se hacen $k$ pasadas con el conjunto inicial de los $n$ casos de prueba brindados por el generador, pues en la aleatoriedad de seleccionar el candidato a eliminar, puede que no siempre se esté caminando hacia el óptimo global. Por lo que varias corridas lanzarán un mejor rendimiento del algoritmo, el cual se quedará con la mejor de todas. La estimación del parámetro $k$ se hace de forma experimental y es posible que dos pasadas brinden la misma solución, pero esto último es muy poco probable.
		
	\section{Adaptación del Genético}
		En (hacer referencia a la subsección), el Algoritmo Genético define varios operadores genéticos a efectuar sobre una población inicial de individuos, que serán métodos que reciben dos soluciones al problema. La entrada al procedimiento será el mismo subconjunto de tamaño $n$ que recibe GRASP. La diferencia está en que este último solo da pasos de tamaño $1$ en cada iteración, pues se visita una solución vecina que difiere en solo un caso ($1$ bit si se ven como máscaras booleanas sobre un conjunto).
		
		La población inicial del algoritmo será de los $n$ casos iniciales, $k, k > 4$ configuraciones diferentes de bits sobre esos $n$. Una vez conformada, se procede a la selección de $4$ individuos que pudieran ser representativos.
		
		\begin{itemize}
			\item individuo $o_1$: el que mejor evalúa la función objetivo.
			\item individuo $o_2$: el segundo que mejor evalúa la función objetivo.
			\item individuos $r_1, r_2$: dos seleccionados de forma aleatoria.
		\end{itemize} 
	
		Operadores genéticos definidos donde $i1, i2$ son individuos de la población, o sea, soluciones al problema.
			\begin{itemize}
			\item operador $op_1(i_1, i_2)$: mezcla la primera mitad de $i_1$ con la segunda mitad de $i_2$.
			\item operador $op_2(i_1, i_2)$: mezcla de forma alternada $i_1$ e $i_2$.
			\item operador $op_3(i_1, i_2)$: unión de miembros aleatorios de $i_1$ e $i_2$.
			\item operador $op_4(i_1, i_2, p)$: mezcla el $p\%$ de $i_1$ con el $(1-p)\%$ de $i_2$.
		\end{itemize}
	
		Una vez definida la población inicial y los operadores genéticos, se procede a aplicar estos últimos sobre los $4$ individuos de la siguiente forma. Dados $o_1$ y $o_2$ se le pasan como parámetros cada operador genético, similar con $r_1$ y $r_2$. De esta forma se obtienen $8$ nuevos individuos, de los cuales se seleccionan los $4$ mejores y se añaden a la población. Esto se repite un número determinado de iteraciones y se devuelve el mejor individuo de la población, el cual minimiza la evaluación de la función objetivo.
		
		Note que en cada iteración del algoritmo, se aumenta en $4$ el número de soluciones, por lo que debe estar regulado el número de las mismas.
	
		De esta forma se intenta cubrir varios caminos que nos puedan llevar al óptimo y se dan pasos de más de tamaño $1$ (que difieran en más de un bit dos soluciones). Con un número de iteraciones lo suficientemente grande, este algoritmo debe acercarse a una buena solución del problema dado el balance entre exploración y explotación que posee.
		
		Falta responder: ¿Cómo estos operadores genéticos resuelven el problema del estancamiento local?
	
	

\bibliography{bibliography}
\bibliographystyle{ieeetr}

\end{document}